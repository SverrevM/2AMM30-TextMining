{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54211863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Sverre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from spacy import Language, util\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n",
    "from transformers import pipeline\n",
    "import crosslingual_coreference\n",
    "import spacy\n",
    "from os.path import isfile\n",
    "import os\n",
    "import ftfy\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb351b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6550955",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"preprocessed-rebel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3b345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_wiki_api(item):\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "        data = requests.get(url).json()\n",
    "        # Return the first id (Could upgrade this in the future)\n",
    "        return data['search'][0]['id']\n",
    "    except:\n",
    "        return 'id-less'\n",
    "\n",
    "def extract_triplets(text):\n",
    "    \"\"\"\n",
    "    Function to parse the generated text and extract the triplets\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29142065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cebba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": 0,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "            Doc.set_extension(\"rel\", default={})\n",
    "   \n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "            return mapping\n",
    "        else:\n",
    "            res = call_wiki_api(item)\n",
    "            self.entity_mapping[item] = res\n",
    "            return res\n",
    "\n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd850c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Sverre\\AppData\\Local\\Temp\\tmphomqrk1g\\config.json as plain json\n",
      "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RebelComponent at 0x23c01737a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})\n",
    "\n",
    "# Define rel extraction model\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", config={\n",
    "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
    "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bbc7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreff(par):\n",
    "    all_relations = []\n",
    "    # process it\n",
    "    coref_text = coref(par)._.resolved_text\n",
    "    print(coref_text)\n",
    "    doc = rel_ext(coref_text)  \n",
    "    print(doc._.rel.items())\n",
    "    for value, rel_dict in doc._.rel.items():\n",
    "        all_relations.append(rel_dict)\n",
    "    \n",
    "    return all_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c86486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        Function to apply preprocessing on a selection of files and store it in a separate folder\n",
    "        - path: root folder (preprocessed-rebel), \n",
    "        - subf desired subfolder: AA or AB, must be passed as a string (e.g. \"AA\") \n",
    "        - files in subfolder AA: e.g. p_r_wiki_00 \n",
    "        - files in subfolder AB: e.g. p_r_wiki_00\n",
    "        - start: start number file, e.g. 0-99 (no need to fill in 00, 0 is fine)\n",
    "        - end: end number file, e.g. 0-99\n",
    "        - the range is inclusive which means, e.g. with (0, 0) you select & pre-process file wiki_00,\n",
    "        - with (32, 50) you select file wiki_32 up till wiki_50\n",
    "\"\"\" \n",
    "def rel_extraction_mul_files(path, subf=None, start=None, end=None):\n",
    "    relations = {}\n",
    "    if subf:\n",
    "        # from file start to end\n",
    "        for i in range(start, end+1):\n",
    "            # to match the filename p_r_wiki_00 up till p_r_wiki_09 we add a zero in front of the number from user input if necessary\n",
    "            if i < 10:\n",
    "                i = \"0\" + str(i)\n",
    "            # construct path to file name that falls within range\n",
    "            f = path + subf + \"/p_r_wiki_{}\".format(i) \n",
    "            \n",
    "            # if file exists\n",
    "            if isfile(f):\n",
    "                # OPEN FILE, GO THROUGH EACH PAR AND PASS THAT INTO COREF FUNCTION\n",
    "                file = open(f, 'r', encoding='utf-8')\n",
    "                doc = json.load(file)\n",
    "                for k, v in tqdm(doc.items()):\n",
    "                    v = ftfy.fix_text(v) # FIX ANY ENCODINGS\n",
    "                    # if paragraph has more than one word\n",
    "                    if len(v.split(\" \")) > 1:\n",
    "                        relations[k] = coreff(v)\n",
    "                        #TODO: FORMAT OUTPUT -> KADIR\n",
    "    else:\n",
    "        for f in glob.glob('preprocessed-rebel/*/*'):\n",
    "            # OPEN EACH FILE, GO THROUGH EACH PARAGRAPH AND PASS THAT INTO COREF FUNC\n",
    "            file = open(f, 'r')\n",
    "            doc = json.load(file)\n",
    "            for k, v in doc.items():\n",
    "                v = ftfy.fix_text(v) # FIX ANY ENCODINGS\n",
    "                # if paragraph has more than one word\n",
    "                if len(v.split(\" \")) > 1:\n",
    "                    relations[k] = coreff(v)\n",
    "                    #TODO: FORMAT OUTPUT -> KADIR\n",
    "                    \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddb80067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/802 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aztlan Underground is a band from Los Angeles, California that combines Hip-Hop, Punk Rock, Jazz, and electronic music with Chicano and Native American themes, and indigenous instrumentation. Aztlan Underground are often cited as progenitors of Chicano rap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/802 [00:09<2:08:25,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('bdfbd12e40515f8d2f5f3529a81303dbd4598fda', {'relation': 'location of formation', 'head_span': {'text': 'Aztlan Underground', 'id': 'Q4832994'}, 'tail_span': {'text': 'Los Angeles', 'id': 'Q65'}}), ('9bda233d90607745443ab7c58c5f94548e4189f5', {'relation': 'genre', 'head_span': {'text': 'Aztlan Underground', 'id': 'Q4832994'}, 'tail_span': {'text': 'Chicano rap', 'id': 'Q1399695'}})])\n",
      "The band traces The band's roots to the late-1980s hardcore scene in the Eastside of Los Angeles. The band have played rapcore, with elements of punk, hip hop, rock, funk, jazz, indigenous music, and spoken word. Indigenous drums, flutes, and rattles are also commonly used in The band's music. The band's lyrics often address the family and economic issues faced by the Chicano community, and The band have been noted as activists for the Chicano community.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/802 [00:23<2:38:31, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('6b09b25f619449ce5e14a281d30bce8c72b9bf08', {'relation': 'located in the administrative territorial entity', 'head_span': {'text': 'Eastside', 'id': 'Q55975144'}, 'tail_span': {'text': 'Los Angeles', 'id': 'Q65'}}), ('3f58fc0f84bd2aab94831cfe5551b20269a3076b', {'relation': 'genre', 'head_span': {'text': 'rapcore', 'id': 'Q3930216'}, 'tail_span': {'text': 'hip hop', 'id': 'Q1132127'}}), ('41f616b3d214629c6b8566bd2d1e0a33ace3fe5c', {'relation': 'subclass of', 'head_span': {'text': 'rattles', 'id': 'Q2132068'}, 'tail_span': {'text': 'drum', 'id': 'Q11404'}}), ('5b72f81ba38ee399ed49b2e9ac7bce220c00c2d1', {'relation': 'ethnic group', 'head_span': {'text': 'Chicano', 'id': 'Q581921'}, 'tail_span': {'text': 'Chicano community', 'id': 'Q113243605'}})])\n",
      "As an example of the politically active and culturally important artists in Los Angeles in the 1990s, Aztlan Underground appeared on \"Culture Clash\" on Fox in 1993; and was part of \"Breaking Out\", a concert on pay per view in 1998, Aztlan Underground was featured in the independent films \"Algun Dia\" and \"Frontierland\" in the 1990s, and on the upcoming \"Studio 49\". Aztlan Underground has been mentioned or featured in various newspapers and magazines: \"the Vancouver Sun\", \"New Times\", \"BLU Magazine\" (an underground hip hop magazine), \"BAM Magazine\", \"La Banda Elastica Magazine\", and the \"Los Angeles Times\" calendar section. Aztlan Underground is also the subject of a chapter in the book \"It is Not About a Salary\", by Brian Cross.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/802 [00:36<2:48:13, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('d0aa9d65542b23b7f9aa1a2354b01000214c7ba5', {'relation': 'original broadcaster', 'head_span': {'text': 'Culture Clash', 'id': 'Q5193269'}, 'tail_span': {'text': 'Fox', 'id': 'Q166419'}}), ('986517f1163473cba3db92ba8d4306d6603ea2f1', {'relation': 'main subject', 'head_span': {'text': 'BLU Magazine', 'id': 'id-less'}, 'tail_span': {'text': 'underground hip hop', 'id': 'Q965635'}}), ('3a6d69252bc880ea374ea13445d5f3ab6c282555', {'relation': 'author', 'head_span': {'text': 'It is Not About a Salary', 'id': 'id-less'}, 'tail_span': {'text': 'Brian Cross', 'id': 'Q19842375'}})])\n",
      "Aztlan Underground remains active in the community, lending Aztlan Underground's voice to annual events such as The Farce of July, and the recent movement to recognize Indigenous People's Day in Los Angeles and beyond.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/802 [00:40<2:04:05,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('520fed7029097ee245b6e50f09fd3c2c6709a7fd', {'relation': 'performer', 'head_span': {'text': 'The Farce of July', 'id': 'id-less'}, 'tail_span': {'text': 'Aztlan Underground', 'id': 'Q4832994'}})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/802 [00:41<2:18:24, 10.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sverre\\Documents\\UNI\\2amm30\\2AMM30-TextMining\\rebel.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# OPT 1: SELECT SPECIFIC FILES TO FEED REBEL IN ONE PARTICULAR MAP / # OPT 2: FEED ALL FILES, in AA and AB BY ONLY KEEPING PATH IN THERE\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m relations \u001b[39m=\u001b[39m rel_extraction_mul_files(path, \u001b[39m\"\u001b[39;49m\u001b[39mAA\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Sverre\\Documents\\UNI\\2amm30\\2AMM30-TextMining\\rebel.ipynb Cell 10\u001b[0m in \u001b[0;36mrel_extraction_mul_files\u001b[1;34m(path, subf, start, end)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                 \u001b[39m# if paragraph has more than one word\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(v\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                     relations[k] \u001b[39m=\u001b[39m coreff(v)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                     \u001b[39m#TODO: FORMAT OUTPUT -> KADIR\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39mpreprocessed-rebel/*/*\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m# OPEN EACH FILE, GO THROUGH EACH PARAGRAPH AND PASS THAT INTO COREF FUNC\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Sverre\\Documents\\UNI\\2amm30\\2AMM30-TextMining\\rebel.ipynb Cell 10\u001b[0m in \u001b[0;36mcoreff\u001b[1;34m(par)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m all_relations \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# process it\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m coref_text \u001b[39m=\u001b[39m coref(par)\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mresolved_text\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(coref_text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sverre/Documents/UNI/2amm30/2AMM30-TextMining/rebel.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m doc \u001b[39m=\u001b[39m rel_ext(coref_text)  \n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1010\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1009\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1010\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1012\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\crosslingual_coreference\\CrossLingualPredictorSpacy.py:33\u001b[0m, in \u001b[0;36mCrossLingualPredictorSpacy.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc: Doc) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[0;32m     24\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m    predict the class for a spacy Doc\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m        Doc: spacy doc with ._.cats key-class proba-value dict\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mpredict(doc\u001b[39m.\u001b[39;49mtext)\n\u001b[0;32m     34\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massign_prediction_to_doc(doc, prediction)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\crosslingual_coreference\\CrossLingualPredictor.py:111\u001b[0m, in \u001b[0;36mCrossLingualPredictor.predict\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39m# make predictions for individual chunks\u001b[39;00m\n\u001b[0;32m    110\u001b[0m json_batch \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m: chunk} \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks]\n\u001b[1;32m--> 111\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49mpredict_batch_json(json_batch)\n\u001b[0;32m    113\u001b[0m \u001b[39m# determine doc_lengths to resolve overlapping chunks\u001b[39;00m\n\u001b[0;32m    114\u001b[0m doc_lengths \u001b[39m=\u001b[39m [\n\u001b[0;32m    115\u001b[0m     \u001b[39msum\u001b[39m([\u001b[39mlen\u001b[39m(sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(doc_chunk\u001b[39m.\u001b[39msents)[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]]) \u001b[39mfor\u001b[39;00m doc_chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39m_spacy\u001b[39m.\u001b[39mpipe(chunks)\n\u001b[0;32m    116\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\allennlp\\predictors\\predictor.py:292\u001b[0m, in \u001b[0;36mPredictor.predict_batch_json\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_batch_json\u001b[39m(\u001b[39mself\u001b[39m, inputs: List[JsonDict]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[JsonDict]:\n\u001b[0;32m    291\u001b[0m     instances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_json_to_instances(inputs)\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_batch_instance(instances)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\allennlp\\predictors\\predictor.py:297\u001b[0m, in \u001b[0;36mPredictor.predict_batch_instance\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39mfor\u001b[39;00m instance \u001b[39min\u001b[39;00m instances:\n\u001b[0;32m    296\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_reader\u001b[39m.\u001b[39mapply_token_indexers(instance)\n\u001b[1;32m--> 297\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mforward_on_instances(instances)\n\u001b[0;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize(outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\allennlp\\models\\model.py:217\u001b[0m, in \u001b[0;36mModel.forward_on_instances\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    215\u001b[0m dataset\u001b[39m.\u001b[39mindex_instances(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m    216\u001b[0m model_input \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mmove_to_device(dataset\u001b[39m.\u001b[39mas_tensor_dict(), cuda_device)\n\u001b[1;32m--> 217\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_output_human_readable(\u001b[39mself\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_input))\n\u001b[0;32m    219\u001b[0m instance_separated_output: List[Dict[\u001b[39mstr\u001b[39m, numpy\u001b[39m.\u001b[39mndarray]] \u001b[39m=\u001b[39m [\n\u001b[0;32m    220\u001b[0m     {} \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39minstances\n\u001b[0;32m    221\u001b[0m ]\n\u001b[0;32m    222\u001b[0m \u001b[39mfor\u001b[39;00m name, output \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(outputs\u001b[39m.\u001b[39mitems()):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\allennlp_models\\coref\\models\\coref.py:280\u001b[0m, in \u001b[0;36mCoreferenceResolver.forward\u001b[1;34m(self, text, spans, span_labels, metadata)\u001b[0m\n\u001b[0;32m    276\u001b[0m top_antecedent_embeddings \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mbatched_index_select(\n\u001b[0;32m    277\u001b[0m     top_span_embeddings, top_antecedent_indices, flat_top_antecedent_indices\n\u001b[0;32m    278\u001b[0m )\n\u001b[0;32m    279\u001b[0m \u001b[39m# Shape: (batch_size, num_spans_to_keep, 1 + max_antecedents)\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m coreference_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_coreference_scores(\n\u001b[0;32m    281\u001b[0m     top_span_embeddings,\n\u001b[0;32m    282\u001b[0m     top_antecedent_embeddings,\n\u001b[0;32m    283\u001b[0m     top_partial_coreference_scores,\n\u001b[0;32m    284\u001b[0m     top_antecedent_mask,\n\u001b[0;32m    285\u001b[0m     top_antecedent_offsets,\n\u001b[0;32m    286\u001b[0m )\n\u001b[0;32m    288\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_order \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    289\u001b[0m     dummy_mask \u001b[39m=\u001b[39m top_antecedent_mask\u001b[39m.\u001b[39mnew_ones(batch_size, num_spans_to_keep, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\allennlp_models\\coref\\models\\coref.py:858\u001b[0m, in \u001b[0;36mCoreferenceResolver._compute_coreference_scores\u001b[1;34m(self, top_span_embeddings, top_antecedent_embeddings, top_partial_coreference_scores, top_antecedent_mask, top_antecedent_offsets)\u001b[0m\n\u001b[0;32m    852\u001b[0m span_pair_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_span_pair_embeddings(\n\u001b[0;32m    853\u001b[0m     top_span_embeddings, top_antecedent_embeddings, top_antecedent_offsets\n\u001b[0;32m    854\u001b[0m )\n\u001b[0;32m    856\u001b[0m \u001b[39m# Shape: (batch_size, num_spans_to_keep, max_antecedents)\u001b[39;00m\n\u001b[0;32m    857\u001b[0m antecedent_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_antecedent_scorer(\n\u001b[1;32m--> 858\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_antecedent_feedforward(span_pair_embeddings)\n\u001b[0;32m    859\u001b[0m )\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    860\u001b[0m antecedent_scores \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m top_partial_coreference_scores\n\u001b[0;32m    861\u001b[0m antecedent_scores \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mreplace_masked_values(\n\u001b[0;32m    862\u001b[0m     antecedent_scores, top_antecedent_mask, util\u001b[39m.\u001b[39mmin_value_of_dtype(antecedent_scores\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    863\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\allennlp\\modules\\time_distributed.py:51\u001b[0m, in \u001b[0;36mTimeDistributed.forward\u001b[1;34m(self, pass_through, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reshape_tensor(value)\n\u001b[0;32m     49\u001b[0m     reshaped_kwargs[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m---> 51\u001b[0m reshaped_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_module(\u001b[39m*\u001b[39mreshaped_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mreshaped_kwargs)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m some_input \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo input tensor to time-distribute\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\allennlp\\modules\\feedforward.py:108\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    104\u001b[0m output \u001b[39m=\u001b[39m inputs\n\u001b[0;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m layer, activation, dropout \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_linear_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activations, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dropout\n\u001b[0;32m    107\u001b[0m ):\n\u001b[1;32m--> 108\u001b[0m     output \u001b[39m=\u001b[39m dropout(activation(layer(output)))\n\u001b[0;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# OPT 1: SELECT SPECIFIC FILES TO FEED REBEL IN ONE PARTICULAR MAP / # OPT 2: FEED ALL FILES, in AA and AB BY ONLY KEEPING PATH IN THERE\n",
    "relations = rel_extraction_mul_files(path, \"AA\", 0, 0) # rel_extraction_mul_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbe982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c590d2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Jack was an emperor. Before his ascension to the Chrysanthemum Throne, his personal name his \"imina\" was Hatsusebe\"-shinnō\", also known as Hatsusebe no Waka-sazaki.',\n",
       " 'Jack was an emperor. Before Jack\\'s ascension to the Chrysanthemum Throne, Jack\\'s personal name Jack\\'s \"imina\" was Hatsusebe\"-shinnō\", also known as Hatsusebe no Waka-sazaki.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'Jack was an emperor. Before his ascension to the Chrysanthemum Throne, his personal name his \"imina\" was Hatsusebe\"-shinnō\", also known as Hatsusebe no Waka-sazaki.'\n",
    "\n",
    "t, coref(t)._.resolved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0f643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8826fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec2626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPT 1: SELECT SPECIFIC FILES TO FEED REBEL IN ONE PARTICULAR MAP / # OPT 2: FEED ALL FILES, in AA and AB BY ONLY KEEPING PATH IN THERE\n",
    "#relations = rel_extraction_mul_files(path, \"AA\", 0, 0) # rel_extraction_mul_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdefefe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "36a40502b4c58eaa666807b0a42297727925289edfd6f0a71d83ace7c9e35f56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
