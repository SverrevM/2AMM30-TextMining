{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "348bcd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ship/miniconda3/envs/2amm30/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import ftfy\n",
    "import contractions\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69146c",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61463fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_map={\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"might have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"shall'n't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"will't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"would't\": \"would not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "}\n",
    "\n",
    "def expand_contractions(sent, mapping):\n",
    "    #pattern for matching contraction with their expansions\n",
    "    pattern = re.compile('({})'.format('|'.join(mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_map(contraction):\n",
    "        #using group method to access subgroups of the match\n",
    "        match = contraction.group(0)\n",
    "        #to retain correct case of the word\n",
    "        first_char = match[0]\n",
    "        #find out the expansion\n",
    "        expansion = mapping.get(match) if mapping.get(match) else mapping.get(match.lower())\n",
    "        expansion = first_char + expansion[1:]\n",
    "        return expansion\n",
    "    \n",
    "    #using sub method to replace all contractions with their expansions for a sentence\n",
    "    #function expand_map will be called for every non overlapping occurence of the pattern\n",
    "    expand_sent = pattern.sub(expand_map, sent)\n",
    "    return expand_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4a309",
   "metadata": {},
   "source": [
    "## Keep only articles that have Persons in the title or text and split texts into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2b4b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(datapath):\n",
    "    ner = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "    file = open(f\"{datapath}\", 'r')\n",
    "    texts = file.readlines()\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for line in texts:\n",
    "        fields = json.loads(line)\n",
    "        \n",
    "        # get entity labels based on NER\n",
    "        first_paragraph = fields[\"text\"].split('\\n')[0]\n",
    "        paragraph_entity_labels = [ent.label_ for ent in ner(first_paragraph).ents]\n",
    "        title_entity_labels = [ent.label_ for ent in ner(fields[\"title\"]).ents]\n",
    "        \n",
    "        # remove empty articles and filter on people\n",
    "        if (fields[\"text\"] and 'PERSON' in title_entity_labels) or ('PERSON' in paragraph_entity_labels):\n",
    "            text_data = fields[\"text\"]\n",
    "            # impute encodings\n",
    "            text_data = ftfy.fix_text(text_data)#.replace('\\n', ' ')\n",
    "            # expand contractions\n",
    "            text_data = expand_contractions(text_data, contraction_map)\n",
    "            # remove punctuations\n",
    "            text_data = text_data.translate(str.maketrans('', '', string.punctuation))\n",
    "            \n",
    "            custom_id = 1\n",
    "            text_split = text_data.split('\\n')\n",
    "            for para in text_split:\n",
    "                new_id = fields['id'] + \"-\" + str(custom_id)\n",
    "                dictionary[new_id] = para\n",
    "                custom_id = int(custom_id) + 1\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac6ae522",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = preprocess_data(\"./enwiki20220701-stripped/enwiki20220701-stripped/AA/wiki_00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca54f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140233\n",
      "According to Photius the sixth book of the New History by Ptolemy Hephaestion reported that Thetis burned in a secret place the children she had by Peleus When she had Achilles Peleus noticed tore him from the flames with only a burnt foot and confided him to the centaur Chiron Later Chiron exhumed the body of the Damysus who was the fastest of all the giants removed the ankle and incorporated it into Achilles burnt foot\n"
     ]
    }
   ],
   "source": [
    "print(len(d))\n",
    "print(d['305-14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f527d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'wiki_00'\n",
    "rootpath = \"./\"\n",
    "\n",
    "file_string = f\"{rootpath}/preprocessed_file_{filename}\"\n",
    "with open(file_string, 'wb') as f:\n",
    "    pickle.dump(d, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed4486",
   "metadata": {},
   "source": [
    "## REBEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50ace151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def call_wiki_api(item):\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "        data = requests.get(url).json()\n",
    "        # Return the first id (Could upgrade this in the future)\n",
    "        return data['search'][0]['id']\n",
    "    except:\n",
    "        return 'id-less'\n",
    "\n",
    "def extract_triplets(text):\n",
    "    \"\"\"\n",
    "    Function to parse the generated text and extract the triplets\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": 0,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "            Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "            return mapping\n",
    "        else:\n",
    "            res = call_wiki_api(item)\n",
    "            self.entity_mapping[item] = res\n",
    "            return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a98f65bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'crosslingual_coreference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b16529932262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcrosslingual_coreference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# Number of the GPU, -1 if want to use CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Add coreference resolution model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'crosslingual_coreference'"
     ]
    }
   ],
   "source": [
    "import crosslingual_coreference\n",
    "\n",
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})\n",
    "\n",
    "# Define rel extraction model\n",
    "\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", config={\n",
    "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
    "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90508c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa887500",
   "metadata": {},
   "source": [
    "This REBEL model implementation suffers from being quite slow. On article ID 586 it is extremely slow and on article ID 12 it throws an error.\n",
    "This is likely due to the length of the texts being very long. In order to fix this we can do the following:\n",
    "- Downscale each article to paragraph and/or sentence level.\n",
    "- Filter on the whole set of articles to limit the overall lengths on all articles to some maximum number of words/characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails on d['12'],307 most likely due to lenght of the text\n",
    "# quite slow.... on d['586']\n",
    "input_text = d['586']\n",
    "\n",
    "coref_text = coref(input_text)._.resolved_text\n",
    "\n",
    "doc = rel_ext(coref_text)\n",
    "\n",
    "for value, rel_dict in doc._.rel.items():\n",
    "    print(f\"{value}: {rel_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83007901",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ef6f2",
   "metadata": {},
   "source": [
    "Some limitations encountered through the NLTK modeling:\n",
    "- The relation extraction is limited based on extract_rels subject and object class, which focuses on very standard instances like PERSON, ORGANIZATION and DATE. This implies that there is quite a strong limit on what kind of articles we can talk about.\n",
    "- extract_rels requires us to specify a pattern for which it aims to find relations, in order to extract many types of relations they must then satisfy two things: 1. the relation is portrayed using a specific pattern and 2. all desires relations must be described through some pattern.\n",
    "\n",
    "Some possible benefits:\n",
    "- The word tokenization as well as the pos_tagging (and even the NE_chunk) components are all separate, meaning it may be possible to adapt the pipeline constructed this way to fit different types of pos_tagging, NE_chunking and tokenization to create a variety of pipelines with possibly different behaviors and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pos tagger since its not included by default\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "# enable only if fresh NLTK or lots of issues\n",
    "#nltk.download('all')\n",
    "\n",
    "#pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.sem.relextract import extract_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34730283",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = d['307'].splitlines()\n",
    "use = te[1]\n",
    "use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a96670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentence\n",
    "#use = \"Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires.\"\n",
    "token = word_tokenize(use)\n",
    "# pos_tag sentence\n",
    "pos = pos_tag(token)\n",
    "#pos\n",
    "# chunk sentence\n",
    "chunked = ne_chunk(pos)\n",
    "chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#(any of 'LOCATION', 'ORGANIZATION', 'PERSON', 'DURATION', 'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE')\n",
    "rels = extract_rels('PERSON', 'GPE', chunked, corpus='ace', pattern=re.compile(r'.*\\bborn\\b.*'), window = 100)\n",
    "rels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b87e8",
   "metadata": {},
   "source": [
    "## NER using BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e274d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER step using BERT\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a837c",
   "metadata": {},
   "source": [
    "## Evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e54bc",
   "metadata": {},
   "source": [
    "Initially the plan for the model evaluation was to annotate a set of articles such that we indicated the relations desired in the article. Then by running the article through each pipeline we would get a list of relations found. Evaluation would then consist of cross referencing between the found list from the pipeline and the desired list manually created. Based on the above pipelines, it may be required to downscale this significantly for example by:\n",
    "- Limiting the size of the article to sentence/paragraph based.\n",
    "- Limiting the set of relations sought after to a set of standard or simple relations.\n",
    "\n",
    "For the purpose of closed KE, which the 2 constructed pipelines entail, evaluation would simple be tracking whether all possible relations were found and an accuracy can be computed based on this.\n",
    "For the purpose on open RE, manual crossreferencing may be required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:2amm30]",
   "language": "python",
   "name": "conda-env-2amm30-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "36a40502b4c58eaa666807b0a42297727925289edfd6f0a71d83ace7c9e35f56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
